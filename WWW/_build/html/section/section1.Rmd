---
title: 'Section 1: Reliability'
output:
  html_document:
    highlight: pygments
    theme: flatly
---

It is often important to determine the reliability of a novel index. For instance, if you have some new RAs working in your lab coding images (e.g., judging the positive valence of each image), you probably want to determine the reliability of the raters, or in other words, the agreement between raters. Depending on the index you are using, there are several measure of reliability. Let's explore these different measures!


Which type of reliability?
------------------------------
<img src="http://stanford.edu/class/psych253/section/reliability_chart.png" alt="reliability chart" width="50%">


Cohen's $\kappa$
------------------------------
In our example (from Markus, Ryff, Curhan & Palmersheim’s study of “Well-Being”), **participants answer 32 questions**. Let us focus on coding the narrative answer to just 1 question, “What does it mean to you to have a good life?”; **43 participants answered this question** in total. The researchers determined a number of **categories** (e.g., relations with others, health, family, enjoyment), and the RAs were tasked with coding each participant's narrative based on whether that narrative included a given category ("yes" = 1) or did not ("no" = 0). Let's determine the reliability of 2 RAs' coding of the category "relations with others." That is, let us determine the agreement between raters. 

### Create table of counts
```{r}
ratings <- matrix(c(20, 4, 8, 11),ncol=2,byrow=TRUE)
colnames(ratings) <- c("rater2_n", "rater2_y")
rownames(ratings) <- c("rater1_n", "rater1_y")
ratings <- as.table(ratings)

# Display table with margins (row/col sums)
addmargins(ratings)
```

### Calculate agreement between raters
```{r}
agree = sum(diag(ratings))

# total number of raters
n = margin.table(ratings)

# percent agreement
agree_percent = agree/n
cat('Percent agreement:', agree_percent)
```

The percent agreement between raters is `r agree_percent`.


### Calculate level of agreement due to chance
```{r}
calc_chance = function(num_raters, table) {
  
  # Calculate total
  n = margin.table(table)
  
  # Add margins to the table
  marg = addmargins(table)

  temp = NULL
  for (i in 1:num_raters) {
    # (% time rater 1 said i * % time rater 2 
    # said i = probability both would say i overall)
    temp[i] = (marg[i, num_raters + 1]/n) * (marg[num_raters + 1, i]/n)
    }
  return(sum(temp))
}

chance_percent = calc_chance(2, ratings)
```

The percent agreement due to chance is `r chance_percent`.

### Calculate Cohen's $\kappa$
```{r}
calc_kappa = function(agree_percent, chance_percent){
  kappa = (agree_percent - chance_percent)/(1-chance_percent)
  return(kappa)
}

kappa = calc_kappa(agree_percent, chance_percent)
```

The reliability of the two raters is `r kappa`.


### Calculate Cohen's $\kappa$ from raw scores

First, let's read in some data:
```{r}
d0 = read.csv("http://stanford.edu/class/psych253/data/kappadata1.csv")

# Check out the structure
str(d0)
head(d0)
```
Here, the columns `psych1` and `psych2` correspond to judge 1 and 2 ratings for experiment 1, and `psych3` and `psych4` correspond to judge 1 and 2 ratings for experiment 2. We want to know, how often do the judges agree on their ratings (i.e., put the same thing) vs. disagree (i.e., put different things)?

Let's visualize the dataset quickly, to get a feel for what we're dealing with:
```{r fig.height=3, fig.width=3}
library(gpairs) # if you don't have it, run: install.packages('gpairs')

# just grab first 4 columns of interest (c(1:4)), and plot:
gpairs(d0[, c(1:4)], upper.pars = list(scatter = "lm", verbose = FALSE,
                                       conditional = "barcode", 
                                       mosaic = "mosaic"), 
       lower.pars = list(scatter = "stats", 
                         conditional = "boxplot", verbose = FALSE,
                         mosaic = "mosaic"), 
       stat.pars = list(fontsize = 14, signif = 0.05, 
                        verbose = FALSE, use.color = TRUE, 
                        missing = "missing", just = "centre"))
```

#### Cross-tabulate the dataframe (i.e., get counts from scores)
```{r}
ctab12 = with(d0, table(psych1, psych2))
ctab12

# Add margins
mtab12 = addmargins(ctab12) # this function makes it easy to calculate your margin sums
mtab12
```

Now we can calculate $\kappa$ as above, OR we can use an R function!

#### Calculate Cohen's $\kappa$ with functions

First, let's use the function `cohen.kappa` from the `{psych}` package. Here, we can input either a **cross-tabulated table** of counts, or the **original dataframe** with scores from each rater in separate columns.
```{r}
library(psych)
k1 = cohen.kappa(ctab12); k1
k2 = cohen.kappa(d0[, c('psych1','psych2')]); k2
```
This provides an estimate of $\kappa$, `r k1$kappa`, plus the lower and upper confidence boundaries.

If we have more than 2 raters, or more than 2 experiments (as is the case here!), we can get more than one $\kappa$ score using `cohen.kappa`. This will compute a matrix of $\kappa$s between each pairing of columns. Here we are interested in the agreement between `psych1` and `psych2` (ratings from rater 1 and 2 for experiment 1) and `psych3` and `psych4` (ratings fro rater 1 and 2 for experiment 2).
```{r}
k3 = cohen.kappa(d0[, c('psych1','psych2','psych3','psych4')]); k3
```

Now, let's use the function `kappa2` from the `{irr}` package. Here we must input the **original dataframe**, where items are in the rows, and each rater is in a column. Note that `kappa2` will only work if there are 2 raters!
```{r}
library(irr)
k4 = kappa2(d0[, c('psych1','psych2')]); k4
```
Here we get an estimate of $\kappa$, `r k4$value`, as well as a z-statistic and p-value.

If we have more than 2 raters, we can use `cohen.kappa` (as done above), or we could use the function `kappam.light` from the `{irr}` package. Here we must input the **original dataframe**. Let's pretend that instead of 2 separate experiments, we had 4 separate experimenters for experiment 1.
```{r}
k5 = kappam.light(d0[, c('psych1','psych2','psych3','psych4')]); k5
```


### Study 3 (HW1 #2)
In Study 3, 3 independent raters coded the same set of 100 stimuli (e.g., narratives) on whether each stimulus contained the idea or meaning of three categories, A, B, and C (e.g., family, health, money). 

Let's start by looking at the reliability for raters 1 and 2 for category A (columns `ra1` and `ra2`).
```{r}
# Subset dataframe to just look at study3
study3 = d0[,5:13] 
head(study3)

kap = kappa2(d0[,c('ra1', 'ra2')]); kap
```
Here, we can see that $\kappa$ = `r kap$value` is pretty low (<.2), so we can't even call it "fair."

Now, let's check out the reliability between all 3 raters for category C:
```{r}
kap = kappam.light(d0[,c('rc1', 'rc2', 'rc3')]); kap
``````

### Coding thoughts (HW1 #3)
In a study on “The Effects of Source Certainty on Consumer Involvement and Persuasion” (*J.Consumer Research*, April 2010) by Uma Karmarkar & Zakary Tormala, participants read a persuasive message about a restaurant; they were randomly assigned to one of 8 experimental conditions that varied by source expertise, source certainty and argument quality. The main **dependent variable** was “How much would you be interested in eating at the restaurant?” (i.e., attitude/intention). Suppose that each participant generates 10 thoughts. Each thought generated by **24 participants** (240 thoughts) is coded by **2 raters** as **‘relevant’ or ‘irrelevant’** to the message, and as **‘negative’ (n), ‘neutral (o), or ‘positive’ (p)**. Thus there are 3 possible codes for relevant thoughts, n, o and p, and 3 possible codes for irrelevant thoughts, n0, o0 and p0 (for a total of 6 codes). For relevant thoughts, we'll define the "extent of thoughts" as $n + o + p$, and "valence of thoughts" as $\frac{(p – n)}{Extent}$.

These indices (extent and valence of thought) are calculated in `thought11.r`. Raw data are in a 240 by 2 matrix, `thought12.r`. 

#### Calculate reliability between raters 1 and 2 from raw data
```{r}
# Load in data (use read.table, since this dataset is not in .csv form!)
d1=read.table("http://stanford.edu/class/psych253/data/thought12.r") 
str(d1)
head(d1)

ctab1 = with(d1, table(rater.1, rater.2))
ctab1

ckap1 = cohen.kappa(ctab1); ckap1

# alternatively, use irr
kappa2(d1[,c('rater.1', 'rater.2')])
```

Let's try just estimating $\kappa$ for relevant thoughts
```{r}
ctabrel = ctab1[c(1,3,5),c(1,3,5)]; ctabrel
kapRel=cohen.kappa(ctabrel); kapRel
```

#### Calculate reliability between raters 1 and 2 for scored data
```{r fig.width=4, fig.height=4}
d2=read.table("http://stanford.edu/class/psych253/data/thought11.r") 
str(d2)
head(d2)

# Compute Pearson's correlation & visualize
library(ggplot2)
val_cor=cor.test(d2$valence.1,d2$valence.2); val_cor
qplot(valence.1, valence.2, data = d2, 
      geom = c("point", "smooth"), method="lm") +theme_bw()

extent_cor=cor.test(d2$extent.1,d2$extent.2); extent_cor
qplot(extent.1, extent.2, data = d2, 
      geom = c("point", "smooth"), method="lm") +theme_bw()
```


Intraclass Correlation Coefficient (ICC)
------------------------------
If the ratings are continuous in nature, we can assess reliability with the **intraclass correlation coefficient (ICC)**. Here, there are a few different kinds of designs we might have, that could effect the way we calculate reliability. For instance, we might have a **1-way design**, in which each item is rated by a randomly chosen set of $k$ raters. Or, we might have a **2-way design (with rater as a random effect)**, in which each item is rated by all $k$ raters, who are chosen randomly; here, since rater is a random effect, the reliability estimate generalizes to a population of raters. On the other hand, we could have a **2-way design (with rater as a fixed effect)**, in which a fixed set of $k$ judges rates all the items; in this case, rater is a fixed effect and thus the reliability estimates do NOT generalize.

```{r}
d3 = data.frame(cbind(video1=c(3, 4, 2), video2=c(6,5,7)))
boxplot(d3)
```

Cronbach's $\alpha$
------------------------------
Cronbach's $\alpha$ is a measure of internal consistency for continuous items -- in other words, how closely a set of items are as a group. 
