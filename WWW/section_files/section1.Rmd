---
title: 'Section 1: Reliability'
output:
  html_document:
    highlight: pygments
    theme: flatly
---

It is often important to determine the reliability of a novel index. For instance, if you have some new RAs working in your lab coding images (e.g., judging the positive valence of each image), you probably want to determine the reliability of the raters, or in other words, the agreement between raters. Depending on the index you are using, there are several measure of reliability. Let's explore these different measures!


Which type of reliability?
------------------------------
<img src="http://stanford.edu/class/psych253/section/reliability_chart.png" alt="reliability chart" width="50%">


Cohen's $\kappa$
------------------------------
In our example (from Markus, Ryff, Curhan & Palmersheim’s study of “Well-Being”), **participants answer 32 questions**. Let us focus on coding the narrative answer to just 1 question, “What does it mean to you to have a good life?”; **43 participants answered this question** in total. The researchers determined a number of **categories** (e.g., relations with others, health, family, enjoyment), and the RAs were tasked with coding each participant's narrative based on whether that narrative included a given category ("yes" = 1) or did not ("no" = 0). Let's determine the reliability of 2 RAs' coding of the category "relations with others." That is, let us determine the agreement between raters. 

### Create table of counts
```{r}
ratings <- matrix(c(20, 4, 8, 11),ncol=2,byrow=TRUE)
colnames(ratings) <- c("rater2_n", "rater2_y")
rownames(ratings) <- c("rater1_n", "rater1_y")
ratings <- as.table(ratings)

# Display table with margins (row/col sums)
addmargins(ratings)
```

### Calculate agreement between raters
```{r}
agree = sum(diag(ratings)); agree

# total number of raters
n = margin.table(ratings); n

# percent agreement
agree_percent = agree/n
cat('Percent agreement:', agree_percent)
```

The percent agreement between raters is `r agree_percent`.


### Calculate level of agreement due to chance
```{r}
calc_chance = function(num_raters, table) {
  
  # Calculate total
  n = margin.table(table)
  
  # Add margins to the table
  marg = addmargins(table)

  temp = NULL
  for (i in 1:num_raters) {
    # (% time rater 1 said i * % time rater 2 
    # said i = probability both would say i overall)
    temp[i] = (marg[i, num_raters + 1]/n) * (marg[num_raters + 1, i]/n)
    }
  return(sum(temp))
}

chance_percent = calc_chance(2, ratings); chance_percent
```

The percent agreement due to chance is `r round(chance_percent, 2)`.

### Calculate Cohen's $\kappa$
```{r}
calc_kappa = function(agree_percent, chance_percent){
  kappa = (agree_percent - chance_percent)/(1-chance_percent)
  return(kappa)
}

kappa = calc_kappa(agree_percent, chance_percent); kappa
```

The reliability of the two raters is `r round(kappa, 2)`.


### Calculate Cohen's $\kappa$ from raw scores

First, let's read in some data:
```{r}
d0 = read.csv("http://stanford.edu/class/psych253/data/kappadata1.csv")

# Check out the structure
str(d0)
head(d0)
```
Here, the columns `psych1` and `psych2` correspond to judge 1 and 2 ratings for experiment 1, and `psych3` and `psych4` correspond to judge 1 and 2 ratings for experiment 2. For each experiment the judges were clinicians, and they were diagnosing a sample of 100 patients as 1 (depressed) or 2 (not depressed). We want to know: how often do the judges (clinicians) agree on their ratings (i.e., put the same thing) vs. disagree (i.e., put different things)?

Let's visualize the dataset quickly, to get a feel for what we're dealing with:
```{r fig.height=3, fig.width=3}
library(gpairs) # if you don't have it, run: install.packages('gpairs')

# just grab first 4 columns of interest (c(1:4)), and plot:
gpairs(d0[, c(1:4)], upper.pars = list(scatter = "lm", verbose = FALSE,
                                       conditional = "barcode", 
                                       mosaic = "mosaic"), 
       lower.pars = list(scatter = "stats", 
                         conditional = "boxplot", verbose = FALSE,
                         mosaic = "mosaic"), 
       stat.pars = list(fontsize = 14, signif = 0.05, 
                        verbose = FALSE, use.color = TRUE, 
                        missing = "missing", just = "centre"))
```

#### Cross-tabulate the dataframe (i.e., get counts from scores)
```{r}
ctab12 = with(d0, table(psych1, psych2))
ctab12

# Add margins
mtab12 = addmargins(ctab12) # this function makes it easy to calculate your margin sums
mtab12
```

Now we can calculate $\kappa$ as above, OR we can use an R function! Try calculating it by hand on your own. During section we'll calculate it using functions.

#### Calculate Cohen's $\kappa$ with functions

First, let's use the function `cohen.kappa` from the `{psych}` package. Here, we can input either a **cross-tabulated table** of counts, or the **original dataframe** with scores from each rater in separate columns.
```{r}
library(psych)
k1 = cohen.kappa(ctab12); k1
k2 = cohen.kappa(d0[, c('psych1','psych2')]); k2
```
This provides an estimate of $\kappa$, `r k1$kappa`, plus the lower and upper confidence boundaries.

If we have more than 2 raters, or more than 2 experiments (as is the case here!), we can get more than one $\kappa$ score using `cohen.kappa`. This will compute a matrix of $\kappa$s between each pairing of columns. Here we are interested in the agreement between `psych1` and `psych2` (ratings from rater 1 and 2 for experiment 1) and `psych3` and `psych4` (ratings fro rater 1 and 2 for experiment 2).
```{r}
k3 = cohen.kappa(d0[, c('psych1','psych2','psych3','psych4')]); k3
```

Now, let's use the function `kappa2` from the `{irr}` package. Here we must input the **original dataframe**, where items are in the rows, and each rater is in a column. Note that `kappa2` will only work if there are 2 raters!
```{r}
library(irr)
k4 = kappa2(d0[, c('psych1','psych2')]); k4
```
Here we get an estimate of $\kappa$, `r k4$value`, as well as a z-statistic and p-value.

If we have more than 2 raters, we can use `cohen.kappa` (as done above), or we could use the function `kappam.light` from the `{irr}` package. Here we must input the **original dataframe**. Let's pretend that instead of 2 separate experiments, we had 4 separate experimenters for experiment 1.
```{r}
k5 = kappam.light(d0[, c('psych1','psych2','psych3','psych4')]); k5
```

Cohen's $\kappa$ & Pearson's $r$
------------------------------
### Study 3 (HW1 #2)
In Study 3, 3 independent raters coded the same set of 100 stimuli (e.g., narratives) on whether each stimulus contained the idea or meaning of three categories, A, B, and C (e.g., family, health, money). 

Let's start by looking at the reliability for raters 1 and 2 for category A (columns `ra1` and `ra2`).
```{r fig.height=6, fig.width=6}
# Subset dataframe to just look at study3
study3 = d0[,5:13] 
head(study3)

# Visualize quickly
gpairs(study3, upper.pars = list(scatter = "lm", verbose = FALSE,
                                       conditional = "barcode", 
                                       mosaic = "mosaic"), 
       lower.pars = list(scatter = "stats", 
                         conditional = "boxplot", verbose = FALSE,
                         mosaic = "mosaic"), 
       stat.pars = list(fontsize = 14, signif = 0.05, 
                        verbose = FALSE, use.color = TRUE, 
                        missing = "missing", just = "centre"))

# Calcuate ra1-ra2 reliability
kap = kappa2(d0[,c('ra1', 'ra2')]); kap
```
Here, we can see that $\kappa$ = `r kap$value` is pretty low (<.2), so we can't even call it "fair."

Now, let's check out the reliability between all 3 raters for category C:
```{r}
kap = kappam.light(d0[,c('rc1', 'rc2', 'rc3')]); kap
``````

### Coding thoughts (HW1 #3)
In a study on “The Effects of Source Certainty on Consumer Involvement and Persuasion” (*J.Consumer Research*, April 2010) by Uma Karmarkar & Zakary Tormala, participants read a persuasive message about a restaurant; they were randomly assigned to one of 8 experimental conditions that varied by source expertise, source certainty and argument quality. The main **dependent variable** was “How much would you be interested in eating at the restaurant?” (i.e., attitude/intention). Suppose that each participant generates 10 thoughts. Each thought generated by **24 participants** (240 thoughts) is coded by **2 raters** as **‘relevant’ or ‘irrelevant’** to the message, and as **‘negative’ (n), ‘neutral (o), or ‘positive’ (p)**. Thus there are 3 possible codes for relevant thoughts, n, o and p, and 3 possible codes for irrelevant thoughts, n0, o0 and p0 (for a total of 6 codes). For relevant thoughts, we'll define the "extent of thoughts" as $n + o + p$, and "valence of thoughts" as $\frac{(p – n)}{Extent}$.

These indices (extent and valence of thought) are calculated in `thought11.r`. Raw data are in a 240 by 2 matrix, `thought12.r`. 

#### Calculate reliability between raters 1 and 2 from counts
```{r}
# Load in data (use read.table, since this dataset is not in .csv form!)
d1=read.table("http://stanford.edu/class/psych253/data/thought12.r") 
str(d1)
head(d1)

ctab1 = with(d1, table(rater.1, rater.2))
ctab1

ckap1 = cohen.kappa(ctab1); ckap1

# alternatively, kappa2 from {irr}
kappa2(d1[,c('rater.1', 'rater.2')])
```

Let's try just estimating $\kappa$ for relevant thoughts
```{r}
ctabrel = ctab1[c(1,3,5),c(1,3,5)]; ctabrel
kapRel=cohen.kappa(ctabrel); kapRel
```

#### Calculate reliability between raters 1 and 2 for scored data
```{r fig.width=4, fig.height=4}
d2=read.table("http://stanford.edu/class/psych253/data/thought11.r") 
str(d2)
head(d2)

# Compute Pearson's correlation & visualize
library(ggplot2)
val_cor=cor.test(d2$valence.1,d2$valence.2); val_cor
qplot(valence.1, valence.2, data = d2, 
      geom = c("point", "smooth"), method="lm") +theme_bw()

extent_cor=cor.test(d2$extent.1,d2$extent.2); extent_cor
qplot(extent.1, extent.2, data = d2, 
      geom = c("point", "smooth"), method="lm") +theme_bw()
```

You can do the same for the datasets `thought22.r` and `thought21.r`. As you do this, note that while the correlations change across datasets, Cohen's $\kappa$ remains unchanged. Why is this the case? When trying to understand why kappa didn't change but Pearson's $r$ did, think about how the two are calculated. What do you plug into the equations to compute each?


Test Construction (HW-1 #4)
------------------------------

When constructing a test (e.g., a scale to measure some psychological construct, such as trait anxiety), you often generate a bunch of different items (e.g., questions) that each subject will answer, and then you eventually average together the responses to all the items to arrive at one measure for each subject. In the example here, we have a test that has **9 items**, with scores from **68 subjects** for each item. Our **goal is to construct a better test consisting of a subset of the best items**.


### Read in the data & look at it!
```{r fig.width=6, fig.height=3}
d1 = read.table('http://stanford.edu/class/psych253/data/hw1.4.txt')

# Look at the dataframe
str(d1)
summary(d1)
describe(d1, na.rm=T)

boxplot(d1)
```

**Visualize relationships between items** -- this can be helpful in identifying if any of the items might be *reverse coded* (e.g., if you have a scale measuring trait anxiety, you might want to throw in some items framed to be the opposite (outgoing, careless, etc.).
```{r fig.height=6, fig.width=6}
gpairs(d1, upper.pars = list(scatter = "lm", verbose = FALSE,
                                       conditional = "barcode", 
                                       mosaic = "mosaic"), 
       lower.pars = list(scatter = "stats", 
                         conditional = "boxplot", verbose = FALSE,
                         mosaic = "mosaic"), 
       stat.pars = list(fontsize = 14, signif = 0.05, 
                        verbose = FALSE, use.color = TRUE, 
                        missing = "missing", just = "centre"))
```

It appears that item 7, and possbly items 8 and 9 might be reverse coded.

### Calculate Cronbach's $\alpha$ & item-total correls

First, let's just check out Cronbach's $\alpha$ without reverse coding any items
```{r}
#Combine all 9 scores into 1 scale, without reverse coding any items.
key1.list = list(all=c(1:9)); key1.list
keys1 = make.keys(9, key1.list)

res1 = scoreItems(keys1, d1)

# Check out results
print(res1$item.cor)  		#raw or uncorrected item-total correls
print(res1$alpha)				#Cronbach's alpha

# Compare to ICC alpha (ICC3k)
ICC(d1)[[1]]$ICC[6]
```

#### Calculating Cronbach's $\alpha$ from `lmer()`
Just for practice, let's calculate Cronbach's $\alpha$ from an `lmer()` model! Remember from lecture notes, to calculate the average ICCk, use this equation: $$\frac{k * ICC_{single}}{1 + (k - 1) * ICC_{single}}$$
```{r}
# Load in packages
library(lme4)
library(reshape2)

# Reshape the dataframe to be long
d1_long = d1
d1_long$subject = c(1:68)
d1_long = melt(d1_long, id.vars = 'subject')
str(d1_long)

# Mixed effects lmer, with random effect of "subject" (`subject`), and fixed effect for the "item" (`variable`)
res = lmer(value ~ variable + (1|subject), data=d1_long)
summary(res)

# Calculate the single fixed ICC
alpha = 0.4107/(0.4107+2.7287)

# Now compute the average alpha
alpha_k = (9 * alpha)/(1 + (9 - 1) * alpha); alpha_k
```


#### Reverse coding items
What about if we just reverse code item 7?
```{r fig.width=4, fig.height=4}
#Combine all 9 scores into 1 scale, reverse coding item #7.
key2.list = list(all=c(1:6, -7, 8,9))
keys2 = make.keys(9, key2.list)
res2 = scoreItems(keys2, d1)
print(res2$item.cor)
print(res2$alpha)

#reverse code 7 manually
max(d1) # scale goes from 1 - 10, max=10
d1_rev7 = d1
d1_rev7[,7] = (max(d1) + 1) - d1_rev7[,7]
plot(d1[,7], d1_rev7[,7])

res2b = scoreItems(keys1, d1_rev7)
print(res2b$item.cor)
print(res2b$alpha)
# same results, just no negative sign in the correlations

# ICC again to double check!
ICC(d1_rev7)[[1]]$ICC[6]
```
If we reverse code 7, we improve reliability (i.e., increase Cronbach's $\alpha$). We might also try reversing items 8 and 9 too.

### Calculate Cronbach's $\alpha$ for different constructs

Now if we had designed this test on our own, we'd probably (hopefully!) have some hypotheses about what constructs each of the items measures. Let's imagine that we had 2 constructs to measure in this test, and items 1-4 measured one construct (e.g., introversion), and  items 7-9 measured another (e.g., conscientousness). We can specify that in our key list, and feed that directly to `scoreItems()` to test this!

```{r}
keys.list = list(construct0 = c(1:6,-7,-8,-9), first = 1:4, last=7:9)
keys4 = make.keys(9, keys.list, item.labels=colnames(d1))
res4 = scoreItems(keys4, d1)
print(res4$item.cor)
print(res4$alpha)
```

Here we can see that reverse coding 7-9 actually produces the highest reliability! However, items 7-9 on their own are not very reliable.


Intraclass Correlation Coefficient (ICC)
------------------------------
If the ratings are continuous in nature, we can assess reliability with the **intraclass correlation coefficient (ICC)**. Here, there are a few different kinds of designs we might have, that could effect the way we calculate reliability. For instance, we might have a **1-way design**, in which each item is rated by a randomly chosen set of $k$ raters. Or, we might have a **2-way design (with rater as a random effect)**, in which each item is rated by all $k$ raters, who are chosen randomly; here, since rater is a random effect, the reliability estimate generalizes to a population of raters. On the other hand, we could have a **2-way design (with rater as a fixed effect)**, in which a fixed set of $k$ judges rates all the items; in this case, rater is a fixed effect and thus the reliability estimates do NOT generalize.

Check out more in the tutorial [here](http://web.stanford.edu/class/psych253/tutorials/ICC_from_linearmodels.html), or the Shiny app [here](http://spark.rstudio.com/supsych/icc/)!
